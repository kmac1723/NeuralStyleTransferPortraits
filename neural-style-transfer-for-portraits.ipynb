{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f894b3e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.014642,
     "end_time": "2023-05-01T10:43:01.280353",
     "exception": false,
     "start_time": "2023-05-01T10:43:01.265711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49cb81",
   "metadata": {
    "papermill": {
     "duration": 0.012591,
     "end_time": "2023-05-01T10:43:01.305606",
     "exception": false,
     "start_time": "2023-05-01T10:43:01.293015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ensure we are using the latest version of tfa addons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc6d260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T10:43:01.334757Z",
     "iopub.status.busy": "2023-05-01T10:43:01.334292Z",
     "iopub.status.idle": "2023-05-01T10:43:14.240935Z",
     "shell.execute_reply": "2023-05-01T10:43:14.239117Z"
    },
    "papermill": {
     "duration": 12.926745,
     "end_time": "2023-05-01T10:43:14.244694",
     "exception": false,
     "start_time": "2023-05-01T10:43:01.317949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.7/site-packages (0.19.0)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow_addons) (2.13.3)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow_addons) (23.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d395813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T10:43:14.277636Z",
     "iopub.status.busy": "2023-05-01T10:43:14.276143Z",
     "iopub.status.idle": "2023-05-01T10:43:24.964858Z",
     "shell.execute_reply": "2023-05-01T10:43:24.963235Z"
    },
    "papermill": {
     "duration": 10.707758,
     "end_time": "2023-05-01T10:43:24.968105",
     "exception": false,
     "start_time": "2023-05-01T10:43:14.260347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b41b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T10:43:24.996050Z",
     "iopub.status.busy": "2023-05-01T10:43:24.995208Z",
     "iopub.status.idle": "2023-05-01T10:43:25.002360Z",
     "shell.execute_reply": "2023-05-01T10:43:25.000800Z"
    },
    "papermill": {
     "duration": 0.024626,
     "end_time": "2023-05-01T10:43:25.005470",
     "exception": false,
     "start_time": "2023-05-01T10:43:24.980844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Verbose\n"
     ]
    }
   ],
   "source": [
    "#Make our Exception reporting verbose for error checking\n",
    "%xmode Verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46317e5",
   "metadata": {
    "papermill": {
     "duration": 0.012427,
     "end_time": "2023-05-01T10:43:25.030518",
     "exception": false,
     "start_time": "2023-05-01T10:43:25.018091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configure Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858389ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T10:43:25.058811Z",
     "iopub.status.busy": "2023-05-01T10:43:25.058364Z",
     "iopub.status.idle": "2023-05-01T10:43:29.463406Z",
     "shell.execute_reply": "2023-05-01T10:43:29.461760Z"
    },
    "papermill": {
     "duration": 4.423155,
     "end_time": "2023-05-01T10:43:29.466451",
     "exception": false,
     "start_time": "2023-05-01T10:43:25.043296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: grpc://10.0.0.2:8470\n",
      "Number of replicas: 8\n",
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "#BATCH_SIZE = 10 * strategy.num_replicas_in_sync # reset BATCH SIZE to a single number?\n",
    "BATCH_SIZE = 10\n",
    "    \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc431b",
   "metadata": {
    "papermill": {
     "duration": 0.01255,
     "end_time": "2023-05-01T10:43:29.492510",
     "exception": false,
     "start_time": "2023-05-01T10:43:29.479960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load datasets\n",
    "\n",
    "## Connect to Google Cloud Storage\n",
    "N.B. Need to enable Google CLoud SDK in the Addons menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb85596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T10:43:29.520470Z",
     "iopub.status.busy": "2023-05-01T10:43:29.519954Z",
     "iopub.status.idle": "2023-05-01T10:43:29.704717Z",
     "shell.execute_reply": "2023-05-01T10:43:29.703356Z"
    },
    "papermill": {
     "duration": 0.202352,
     "end_time": "2023-05-01T10:43:29.707720",
     "exception": false,
     "start_time": "2023-05-01T10:43:29.505368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8085ee3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-01T10:43:29.735949Z",
     "iopub.status.busy": "2023-05-01T10:43:29.735499Z",
     "iopub.status.idle": "2023-05-01T10:43:30.659683Z",
     "shell.execute_reply": "2023-05-01T10:43:30.655994Z"
    },
    "papermill": {
     "duration": 0.941693,
     "end_time": "2023-05-01T10:43:30.662514",
     "exception": true,
     "start_time": "2023-05-01T10:43:29.720821",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BackendError",
     "evalue": "Unexpected response from the service. Response: {'errors': ['Google Cloud SDK must be authorized before copying private datasets.'], 'error': {'code': 9, 'details': []}, 'wasSuccessful': False}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20/1176666021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGCS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gcs_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mGCS_PATH\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mKaggleDatasets.get_gcs_path\u001b[0m \u001b[0;34m= <function KaggleDatasets.get_gcs_path at 0x7004e75f6440>\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kaggle_datasets.py\u001b[0m in \u001b[0;36mget_gcs_path\u001b[0;34m(self=<kaggle_datasets.KaggleDatasets object>, dataset_dir=None)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;34m'IntegrationType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mintegration_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         }\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_post_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_GCS_PATH_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIMEOUT_SECS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.web_client.make_post_request\u001b[0m \u001b[0;34m= <bound method KaggleWebClient.make_post_request of <kaggle_web_client.KaggleWebClient object at 0x7004e75681d0>>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mdata\u001b[0m \u001b[0;34m= {'MountSlug': None, 'IntegrationType': 2}\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.GET_GCS_PATH_ENDPOINT\u001b[0m \u001b[0;34m= '/requests/CopyDatasetVersionToKnownGcsBucketRequest'\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.TIMEOUT_SECS\u001b[0m \u001b[0;34m= 600\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'destinationBucket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kaggle_web_client.py\u001b[0m in \u001b[0;36mmake_post_request\u001b[0;34m(self=<kaggle_web_client.KaggleWebClient object>, data={'IntegrationType': 2, 'MountSlug': None}, endpoint='/requests/CopyDatasetVersionToKnownGcsBucketRequest', timeout=600)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wasSuccessful'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'result'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     raise BackendError(\n\u001b[0;32m---> 50\u001b[0;31m                         f'Unexpected response from the service. Response: {response_json}.')\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBackendError\u001b[0m: Unexpected response from the service. Response: {'errors': ['Google Cloud SDK must be authorized before copying private datasets.'], 'error': {'code': 9, 'details': []}, 'wasSuccessful': False}."
     ]
    }
   ],
   "source": [
    "GCS_PATH = KaggleDatasets().get_gcs_path()\n",
    "print(GCS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c14d03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736927d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:27:07.364001Z",
     "iopub.status.busy": "2022-10-09T12:27:07.363345Z",
     "iopub.status.idle": "2022-10-09T12:27:07.373347Z",
     "shell.execute_reply": "2022-10-09T12:27:07.372295Z",
     "shell.execute_reply.started": "2022-10-09T12:27:07.363962Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [256, 256] # amend our image size to regular\n",
    "OUTPUT_CHANNELS = 3\n",
    "BATCH_SIZE =  10\n",
    "EPOCHS_NUM = 10\n",
    "\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.image.resize(image, [*IMAGE_SIZE])\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    return image\n",
    "\n",
    "# This function extracts the image data from the files to create our dataset\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames\n",
    "    )  # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(\n",
    "        ignore_order\n",
    "    )  # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(\n",
    "        partial(read_tfrecord), num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or just images if labeled=False\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee650ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:20:17.256773Z",
     "iopub.status.busy": "2022-10-09T12:20:17.256363Z",
     "iopub.status.idle": "2022-10-09T12:20:17.265172Z",
     "shell.execute_reply": "2022-10-09T12:20:17.264074Z",
     "shell.execute_reply.started": "2022-10-09T12:20:17.256738Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gan_dataset(source_files, target_files, augment=None, repeat=True, shuffle=True, batch_size=1):\n",
    "\n",
    "    source_ds = load_dataset(source_files)\n",
    "    target_ds = load_dataset(target_files)\n",
    "    \n",
    "    if augment:\n",
    "        source_ds = source_ds.map(augment, num_parallel_calls=AUTO)\n",
    "        target_ds = target_ds.map(augment, num_parallel_calls=AUTO)\n",
    "        \n",
    "    if repeat:\n",
    "        source_ds = source_ds.repeat()\n",
    "        target_ds = target_ds.repeat()\n",
    "    if shuffle:\n",
    "        source_ds = source_ds.shuffle(2048)\n",
    "        target_ds = target_ds.shuffle(2048)\n",
    "        \n",
    "    source_ds = source_ds.batch(batch_size, drop_remainder=True)\n",
    "    target_ds = target_ds.batch(batch_size, drop_remainder=True)\n",
    "    #source_ds = source_ds.cache()\n",
    "    #target_ds = target_ds.cache()\n",
    "    source_ds = source_ds.prefetch(AUTOTUNE)\n",
    "    target_ds = target_ds.prefetch(AUTOTUNE)\n",
    "    \n",
    "    gan_ds = tf.data.Dataset.zip((source_ds, target_ds))\n",
    "    \n",
    "    return gan_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b797a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Load our source and target datasets\n",
    "I.E. what style of images we are converting from and to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e518958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:20:25.637276Z",
     "iopub.status.busy": "2022-10-09T12:20:25.634707Z",
     "iopub.status.idle": "2022-10-09T12:20:28.002337Z",
     "shell.execute_reply": "2022-10-09T12:20:28.001199Z",
     "shell.execute_reply.started": "2022-10-09T12:20:25.637222Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_path = '/art_portraits_tfrec/photo_tfrec/*.tfrec'\n",
    "target_path = '/art_portraits_tfrec/ukiyo_tfrec/*.tfrec'\n",
    "\n",
    "SOURCE_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + source_path))\n",
    "print('Source TFRecord Files:', len(SOURCE_FILENAMES))\n",
    "\n",
    "TARGET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + target_path))\n",
    "print('Target TFRecord Files:', len(TARGET_FILENAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953ae97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:20:33.399882Z",
     "iopub.status.busy": "2022-10-09T12:20:33.399482Z",
     "iopub.status.idle": "2022-10-09T12:20:33.611375Z",
     "shell.execute_reply": "2022-10-09T12:20:33.610571Z",
     "shell.execute_reply.started": "2022-10-09T12:20:33.399845Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_ds = load_dataset(SOURCE_FILENAMES, labeled=False).batch(1)\n",
    "target_ds = load_dataset(TARGET_FILENAMES, labeled=False).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99517a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:20:36.347522Z",
     "iopub.status.busy": "2022-10-09T12:20:36.346626Z",
     "iopub.status.idle": "2022-10-09T12:20:36.404013Z",
     "shell.execute_reply": "2022-10-09T12:20:36.402876Z",
     "shell.execute_reply.started": "2022-10-09T12:20:36.347478Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will also load some datasets for FID implementation\n",
    "fid_source_ds = load_dataset(SOURCE_FILENAMES).take(1024).batch(32*strategy.num_replicas_in_sync).prefetch(32)\n",
    "fid_target_ds = load_dataset(TARGET_FILENAMES).batch(32*strategy.num_replicas_in_sync).prefetch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3ab7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:20:39.371683Z",
     "iopub.status.busy": "2022-10-09T12:20:39.371239Z",
     "iopub.status.idle": "2022-10-09T12:20:39.436629Z",
     "shell.execute_reply": "2022-10-09T12:20:39.435342Z",
     "shell.execute_reply.started": "2022-10-09T12:20:39.371646Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_dataset = get_gan_dataset(SOURCE_FILENAMES, TARGET_FILENAMES, augment=None, repeat=True, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447743fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Check that our data has loaded as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bf5a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:20:42.764137Z",
     "iopub.status.busy": "2022-10-09T12:20:42.763673Z",
     "iopub.status.idle": "2022-10-09T12:20:47.900709Z",
     "shell.execute_reply": "2022-10-09T12:20:47.899480Z",
     "shell.execute_reply.started": "2022-10-09T12:20:42.764097Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "buffer_dataset = iter(full_dataset)\n",
    "example_source , example_target = next(buffer_dataset)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Source')\n",
    "plt.imshow(example_source[0] * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('target')\n",
    "plt.imshow(example_target[0] * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f7873",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Using Frechet Inception Distance as a metric\n",
    "FID is a metric that calculates the distance between feature vectors the distribution of the generated image against the real ones that were used to train the generator. We calculate this by using a pre-trained inception model for image classification. With FID, the lower the score, the more similiar the images are i.e. the better the GAN is at producing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485f9a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:21:06.995488Z",
     "iopub.status.busy": "2022-10-09T12:21:06.995095Z",
     "iopub.status.idle": "2022-10-09T12:21:52.889803Z",
     "shell.execute_reply": "2022-10-09T12:21:52.888762Z",
     "shell.execute_reply.started": "2022-10-09T12:21:06.995455Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    inception_model = tf.keras.applications.InceptionV3(input_shape=(256,256,3),pooling=\"avg\",include_top=False) # amended input shape\n",
    "\n",
    "    mix3  = inception_model.get_layer(\"mixed9\").output\n",
    "    f0 = tf.keras.layers.GlobalAveragePooling2D()(mix3)\n",
    "\n",
    "    inception_model = tf.keras.Model(inputs=inception_model.input, outputs=f0)\n",
    "    inception_model.trainable = False\n",
    "\n",
    "    \n",
    "    \n",
    "    def calculate_activation_statistics_mod(images,fid_model):\n",
    "\n",
    "            act=tf.cast(fid_model.predict(images), tf.float32)\n",
    "\n",
    "            mu = tf.reduce_mean(act, axis=0)\n",
    "            mean_x = tf.reduce_mean(act, axis=0, keepdims=True)\n",
    "            mx = tf.matmul(tf.transpose(mean_x), mean_x)\n",
    "            vx = tf.matmul(tf.transpose(act), act)/tf.cast(tf.shape(act)[0], tf.float32)\n",
    "            sigma = vx - mx\n",
    "            return mu, sigma\n",
    "    myFID_mu2, myFID_sigma2 = calculate_activation_statistics_mod(fid_target_ds,inception_model)        \n",
    "    fids=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728814c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:23:04.223359Z",
     "iopub.status.busy": "2022-10-09T12:23:04.222889Z",
     "iopub.status.idle": "2022-10-09T12:23:04.234778Z",
     "shell.execute_reply": "2022-10-09T12:23:04.233974Z",
     "shell.execute_reply.started": "2022-10-09T12:23:04.223321Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def calculate_frechet_distance(mu1,sigma1,mu2,sigma2):\n",
    "        fid_epsilon = 1e-14\n",
    "       \n",
    "        covmean = tf.linalg.sqrtm(tf.cast(tf.matmul(sigma1,sigma2),tf.complex64))\n",
    "#         isgood=tf.cast(tf.math.is_finite(covmean), tf.int32)\n",
    "#         if tf.size(isgood)!=tf.math.reduce_sum(isgood):\n",
    "#             return 0\n",
    "\n",
    "        covmean = tf.cast(tf.math.real(covmean),tf.float32)\n",
    "  \n",
    "        tr_covmean = tf.linalg.trace(covmean)\n",
    "\n",
    "\n",
    "        return tf.matmul(tf.expand_dims(mu1 - mu2, axis=0),tf.expand_dims(mu1 - mu2, axis=1)) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def FID(images,gen_model,inception_model=inception_model,myFID_mu2=myFID_mu2, myFID_sigma2=myFID_sigma2):\n",
    "                inp = layers.Input(shape=[512, 512, 3], name='input_image') # amended input shape\n",
    "                x  = gen_model(inp)\n",
    "                x=inception_model(x)\n",
    "                fid_model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "                \n",
    "                mu1, sigma1= calculate_activation_statistics_mod(images,fid_model)\n",
    "\n",
    "                fid_value = calculate_frechet_distance(mu1, sigma1,myFID_mu2, myFID_sigma2)\n",
    "\n",
    "\n",
    "                return fid_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21006882",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Building our Generator network\n",
    "We'll be using a UNET architecture for our CycleGAN. (Paper: https://arxiv.org/pdf/1505.04597v1.pdf)\n",
    "\n",
    "First, we define upsample and downsample functions. These will reduce the 2D dimensions of the image by the stride i.e. the size of the steps that the filter takes when performing the convolution/deconvolution.\n",
    "\n",
    "We are also using Instance Normalisation (https://arxiv.org/pdf/1607.08022.pdf) rather than Batch Normalisation, as this is faster than applying normalisation to sets of images and obtains similiar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84baecfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:23:11.875066Z",
     "iopub.status.busy": "2022-10-09T12:23:11.874638Z",
     "iopub.status.idle": "2022-10-09T12:23:11.886308Z",
     "shell.execute_reply": "2022-10-09T12:23:11.884789Z",
     "shell.execute_reply.started": "2022-10-09T12:23:11.875026Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "def downsample(filters, size, apply_instancenorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_instancenorm:\n",
    "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    result.add(layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    result = keras.Sequential()\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "\n",
    "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "\n",
    "    result.add(layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcdae7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "With these functions defined, we can now build our generator model. We do this by downsampling then upsampling the image, whil establishing long skip connections. A skip connection is where we feded the output from an earlier layer into a later one, \"skipping\" some layers. This solves the degradation problem, where adding deeper layers reduces the performance of the model, as deep layers don't learn as effectively as shallow ones (https://www.analyticsvidhya.com/blog/2021/08/all-you-need-to-know-about-skip-connections/). In UNET architecture, our skip connections concatenate the output of each downsample layer onto the input of the equivalent upsample layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8ec32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "TODO: adjust our up and downsample layer to accomodate the new size of our input images.\n",
    "* Could add an additional layer?\n",
    "* Resize the images prior to loading into model?\n",
    "* Or just leave it as is and see what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f888c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:23:17.531981Z",
     "iopub.status.busy": "2022-10-09T12:23:17.531553Z",
     "iopub.status.idle": "2022-10-09T12:23:17.543432Z",
     "shell.execute_reply": "2022-10-09T12:23:17.542172Z",
     "shell.execute_reply.started": "2022-10-09T12:23:17.531943Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = layers.Input(shape=[256,256,3]) # adjusted to 512, 512\n",
    "\n",
    "    # bs = batch size\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_instancenorm=False), # (bs, 256, 256, 32)\n",
    "        downsample(128, 4), # (bs, 128, 128, 64)\n",
    "        downsample(256, 4), # (bs, 64, 64, 128)\n",
    "        downsample(512, 4), # (bs, 32, 32, 256)\n",
    "        downsample(512, 4), # (bs, 16, 16, 512)\n",
    "        downsample(512, 4), # (bs, 8, 8, 512)\n",
    "        downsample(512, 4), # (bs, 4, 4, 512)\n",
    "        downsample(512, 4), # (bs, 2, 2, 512)\n",
    "        #downsample(512, 4), # (bs, 1, 1, 512) # added another downsample layer\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        #upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024) # added another upsample layer\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "        upsample(512, 4, apply_dropout=True), # (bs, 16, 16, 1024)\n",
    "        upsample(512, 4), # (bs, 32, 32, 1024)\n",
    "        upsample(256, 4), # (bs, 64, 64, 512)\n",
    "        upsample(128, 4), # (bs, 128, 128, 256)\n",
    "        upsample(64, 4), # (bs, 256, 256, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                  strides=2,\n",
    "                                  padding='same',\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbe91e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Visualising the Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec44f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d4853",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_viz = Generator()\n",
    "plot_model(gen_viz, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794df712",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_viz.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914f7c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Building the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3a174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:24:52.886951Z",
     "iopub.status.busy": "2022-10-09T12:24:52.885842Z",
     "iopub.status.idle": "2022-10-09T12:24:52.896487Z",
     "shell.execute_reply": "2022-10-09T12:24:52.895348Z",
     "shell.execute_reply.started": "2022-10-09T12:24:52.886878Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    inp = layers.Input(shape=[256, 256, 3], name='input_image') # adjusted to 512x512\n",
    "\n",
    "    x = inp\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x) # (bs, 256, 256, 32)\n",
    "    down2 = downsample(128, 4)(down1) # (bs, 128, 128, 64)\n",
    "    down3 = downsample(256, 4)(down2) # (bs, 64, 64, 128)\n",
    "    #down4 = downsample(512, 4)(down3) # (bs, 32, 32, 256) # added another downsample layer\n",
    "\n",
    "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256) # should have fixed this too?\n",
    "    conv = layers.Conv2D(512, 4, strides=1,\n",
    "                         kernel_initializer=initializer,\n",
    "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
    "\n",
    "    leaky_relu = layers.LeakyReLU()(norm1)\n",
    "\n",
    "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "    last = layers.Conv2D(1, 4, strides=1,\n",
    "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=inp, outputs=last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2a04f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Visuaising the Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca73a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Discriminator_viz = Discriminator()\n",
    "plot_model(Discriminator_viz, show_shapes=True,show_dtype=True,show_layer_names=False , expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b80557",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Discriminator_viz.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e320d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:24:56.329171Z",
     "iopub.status.busy": "2022-10-09T12:24:56.328776Z",
     "iopub.status.idle": "2022-10-09T12:25:01.181936Z",
     "shell.execute_reply": "2022-10-09T12:25:01.180699Z",
     "shell.execute_reply.started": "2022-10-09T12:24:56.329138Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    source_generator = Generator() # transforms source images to the target style\n",
    "    target_generator = Generator() # transforms target style images to be more the source\n",
    "\n",
    "    source_discriminator = Discriminator() # differentiates real source images and generated sourcs\n",
    "    target_discriminator = Discriminator() # differentiates real target style images and generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25382ec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can at least quickly and easily test the Generator contructor without having to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7d3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:25:04.080852Z",
     "iopub.status.busy": "2022-10-09T12:25:04.080453Z",
     "iopub.status.idle": "2022-10-09T12:25:10.057954Z",
     "shell.execute_reply": "2022-10-09T12:25:10.056600Z",
     "shell.execute_reply.started": "2022-10-09T12:25:04.080816Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_target = source_generator(example_source)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(example_source[0] * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Target Image Style\")\n",
    "plt.imshow(to_target[0] * 0.5 + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a833f6b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Image Augmentation\n",
    "We can make the model more data-efficient by using Differential Augmentation, as described here: (https://arxiv.org/pdf/2006.10738.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d8251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:25:29.137218Z",
     "iopub.status.busy": "2022-10-09T12:25:29.136809Z",
     "iopub.status.idle": "2022-10-09T12:25:29.163388Z",
     "shell.execute_reply": "2022-10-09T12:25:29.161978Z",
     "shell.execute_reply.started": "2022-10-09T12:25:29.137184Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
    "# https://arxiv.org/pdf/2006.10738\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "with strategy.scope():\n",
    "    def DiffAugment(x, policy='', channels_first=False):\n",
    "        if policy:\n",
    "            if channels_first:\n",
    "                x = tf.transpose(x, [0, 2, 3, 1])\n",
    "            for p in policy.split(','):\n",
    "                for f in AUGMENT_FNS[p]:\n",
    "                    x = f(x)\n",
    "            if channels_first:\n",
    "                x = tf.transpose(x, [0, 3, 1, 2])\n",
    "        return x\n",
    "\n",
    "\n",
    "    def rand_brightness(x):\n",
    "        magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) - 0.5\n",
    "        x = x + magnitude\n",
    "        return x\n",
    "\n",
    "\n",
    "    def rand_saturation(x):\n",
    "        magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) * 2\n",
    "        x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "        x = (x - x_mean) * magnitude + x_mean\n",
    "        return x\n",
    "\n",
    "\n",
    "    def rand_contrast(x):\n",
    "        magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) + 0.5\n",
    "        x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "        x = (x - x_mean) * magnitude + x_mean\n",
    "        return x\n",
    "\n",
    "\n",
    "    def rand_translation(x, ratio=0.125):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        image_size = tf.shape(x)[1:3]\n",
    "        shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "        translation_x = tf.random.uniform([batch_size, 1], -shift[0], shift[0] + 1, dtype=tf.int32)\n",
    "        translation_y = tf.random.uniform([batch_size, 1], -shift[1], shift[1] + 1, dtype=tf.int32)\n",
    "        grid_x = tf.clip_by_value(tf.expand_dims(tf.range(image_size[0], dtype=tf.int32), 0) + translation_x + 1, 0, image_size[0] + 1)\n",
    "        grid_y = tf.clip_by_value(tf.expand_dims(tf.range(image_size[1], dtype=tf.int32), 0) + translation_y + 1, 0, image_size[1] + 1)\n",
    "        x = tf.gather_nd(tf.pad(x, [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_x, -1), batch_dims=1)\n",
    "        x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3]), [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_y, -1), batch_dims=1), [0, 2, 1, 3])\n",
    "        return x\n",
    "\n",
    "\n",
    "    def rand_cutout(x, ratio=0.5):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        image_size = tf.shape(x)[1:3]\n",
    "        cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "        offset_x = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[0] + (1 - cutout_size[0] % 2), dtype=tf.int32)\n",
    "        offset_y = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[1] + (1 - cutout_size[1] % 2), dtype=tf.int32)\n",
    "        grid_batch, grid_x, grid_y = tf.meshgrid(tf.range(batch_size, dtype=tf.int32), tf.range(cutout_size[0], dtype=tf.int32), tf.range(cutout_size[1], dtype=tf.int32), indexing='ij')\n",
    "        cutout_grid = tf.stack([grid_batch, grid_x + offset_x - cutout_size[0] // 2, grid_y + offset_y - cutout_size[1] // 2], axis=-1)\n",
    "        mask_shape = tf.stack([batch_size, image_size[0], image_size[1]])\n",
    "        cutout_grid = tf.maximum(cutout_grid, 0)\n",
    "        cutout_grid = tf.minimum(cutout_grid, tf.reshape(mask_shape - 1, [1, 1, 1, 3]))\n",
    "        mask = tf.maximum(1 - tf.scatter_nd(cutout_grid, tf.ones([batch_size, cutout_size[0], cutout_size[1]], dtype=tf.float32), mask_shape), 0)\n",
    "        x = x * tf.expand_dims(mask, axis=3)\n",
    "        return x\n",
    "\n",
    "\n",
    "    AUGMENT_FNS = {\n",
    "        'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "        'translation': [rand_translation],\n",
    "        'cutout': [rand_cutout],\n",
    "    }\n",
    "    # To use in our code, we will create a wrapper function and incorporate it into our training step.\n",
    "    def aug_fn(image):\n",
    "         return DiffAugment(image,\"color,translation,cutout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac0819",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Building the CycleGAN\n",
    "Our main CycleGAN class will be doing a lot: we will inherit from the keras.Model class in order to use the.fit() method when training our GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaace57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:25:44.813747Z",
     "iopub.status.busy": "2022-10-09T12:25:44.813055Z",
     "iopub.status.idle": "2022-10-09T12:25:44.834563Z",
     "shell.execute_reply": "2022-10-09T12:25:44.833327Z",
     "shell.execute_reply.started": "2022-10-09T12:25:44.813711Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_generator,\n",
    "        source_generator,\n",
    "        target_discriminator,\n",
    "        source_discriminator,\n",
    "        lambda_cycle=10,\n",
    "    ):\n",
    "        super(CycleGan, self).__init__()\n",
    "        self.m_gen = target_generator\n",
    "        self.p_gen = source_generator\n",
    "        self.m_disc = target_discriminator\n",
    "        self.p_disc = source_discriminator\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        \n",
    "    def compile(\n",
    "        self,\n",
    "        m_gen_optimizer,\n",
    "        p_gen_optimizer,\n",
    "        m_disc_optimizer,\n",
    "        p_disc_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "        cycle_loss_fn,\n",
    "        identity_loss_fn,\n",
    "        jit_compile=False #disables XLA, might fix the problem?\n",
    "    ):\n",
    "        super(CycleGan, self).compile()\n",
    "        self.m_gen_optimizer = m_gen_optimizer\n",
    "        self.p_gen_optimizer = p_gen_optimizer\n",
    "        self.m_disc_optimizer = m_disc_optimizer\n",
    "        self.p_disc_optimizer = p_disc_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = cycle_loss_fn\n",
    "        self.identity_loss_fn = identity_loss_fn\n",
    "        \n",
    "    def train_step(self, batch_data):\n",
    "        # To use DiffAug, we will concatenate the real and fake images, run DiffAug, then separate them out again:\n",
    "        real_target, real_source = batch_data\n",
    "        batch_size = tf.shape(real_target)[0]\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # source to target back to source\n",
    "            fake_target = self.m_gen(real_source, training=True)\n",
    "            cycled_source = self.p_gen(fake_target, training=True)\n",
    "\n",
    "            # target to source back to target\n",
    "            fake_source = self.p_gen(real_target, training=True)\n",
    "            cycled_target = self.m_gen(fake_source, training=True)\n",
    "\n",
    "            # generating itself\n",
    "            same_target = self.m_gen(real_target, training=True)\n",
    "            same_source = self.p_gen(real_source, training=True)\n",
    "            \n",
    "            # Applying DiffAug\n",
    "            both_target = tf.concat([real_target, fake_target], axis=0)            \n",
    "\n",
    "            aug_target = aug_fn(both_target)\n",
    "\n",
    "            aug_real_target = aug_target[:batch_size]\n",
    "            aug_fake_target = aug_target[batch_size:]\n",
    "\n",
    "            # discriminator used to check, inputing real images\n",
    "            disc_real_target = self.m_disc(aug_real_target, training=True) # Use aug_real_target\n",
    "            disc_real_source = self.p_disc(real_source, training=True)\n",
    "\n",
    "            # discriminator used to check, inputing fake images\n",
    "            disc_fake_target = self.m_disc(aug_fake_target, training=True) # Use aug_fake_target\n",
    "            disc_fake_source = self.p_disc(fake_source, training=True)\n",
    "\n",
    "            # evaluates generator loss\n",
    "            target_gen_loss = self.gen_loss_fn(disc_fake_target)\n",
    "            source_gen_loss = self.gen_loss_fn(disc_fake_source)\n",
    "\n",
    "            # evaluates total cycle consistency loss\n",
    "            total_cycle_loss = self.cycle_loss_fn(real_target, cycled_target, self.lambda_cycle) + self.cycle_loss_fn(real_source, cycled_source, self.lambda_cycle)\n",
    "\n",
    "            # evaluates total generator loss\n",
    "            total_target_gen_loss = target_gen_loss + total_cycle_loss + self.identity_loss_fn(real_target, same_target, self.lambda_cycle)\n",
    "            total_source_gen_loss = source_gen_loss + total_cycle_loss + self.identity_loss_fn(real_source, same_source, self.lambda_cycle)\n",
    "\n",
    "            # evaluates discriminator loss\n",
    "            target_disc_loss = self.disc_loss_fn(disc_real_target, disc_fake_target)\n",
    "            source_disc_loss = self.disc_loss_fn(disc_real_source, disc_fake_source)\n",
    "\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        target_generator_gradients = tape.gradient(total_target_gen_loss,\n",
    "                                                  self.m_gen.trainable_variables)\n",
    "        source_generator_gradients = tape.gradient(total_source_gen_loss,\n",
    "                                                  self.p_gen.trainable_variables)\n",
    "\n",
    "        target_discriminator_gradients = tape.gradient(target_disc_loss,\n",
    "                                                      self.m_disc.trainable_variables)\n",
    "        source_discriminator_gradients = tape.gradient(source_disc_loss,\n",
    "                                                      self.p_disc.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        self.m_gen_optimizer.apply_gradients(zip(target_generator_gradients,\n",
    "                                                 self.m_gen.trainable_variables))\n",
    "\n",
    "        self.p_gen_optimizer.apply_gradients(zip(source_generator_gradients,\n",
    "                                                 self.p_gen.trainable_variables))\n",
    "\n",
    "        self.m_disc_optimizer.apply_gradients(zip(target_discriminator_gradients,\n",
    "                                                  self.m_disc.trainable_variables))\n",
    "\n",
    "        self.p_disc_optimizer.apply_gradients(zip(source_discriminator_gradients,\n",
    "                                                  self.p_disc.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            \"target_gen_loss\": total_target_gen_loss,\n",
    "            \"source_gen_loss\": total_source_gen_loss,\n",
    "            \"target_disc_loss\": target_disc_loss,\n",
    "            \"source_disc_loss\": source_disc_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54edf64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Define Loss Functions\n",
    "\n",
    "Each model requires a loss function. Our discriminator will compare real images to a matrix of 1s and fake images to a matrix of 0s, as a perfect discriminator will output pure 1s or 0s for real or fake. We will use the BinaryCrossEntropy loss function for this, and output the average of the real and generated loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738a371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:25:51.659306Z",
     "iopub.status.busy": "2022-10-09T12:25:51.658917Z",
     "iopub.status.idle": "2022-10-09T12:25:51.666634Z",
     "shell.execute_reply": "2022-10-09T12:25:51.665196Z",
     "shell.execute_reply.started": "2022-10-09T12:25:51.659273Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def discriminator_loss(real, generated):\n",
    "        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n",
    "\n",
    "        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
    "\n",
    "        total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "        return total_disc_loss * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c74e72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Our generator is easier: as we are only checking the generated images, which we want the discriminator to evaluate as a matrix of 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f36d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:25:56.144258Z",
     "iopub.status.busy": "2022-10-09T12:25:56.143842Z",
     "iopub.status.idle": "2022-10-09T12:25:56.150517Z",
     "shell.execute_reply": "2022-10-09T12:25:56.149288Z",
     "shell.execute_reply.started": "2022-10-09T12:25:56.144225Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def generator_loss(generated):\n",
    "        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faae088",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "To calculate the cycle consistency loss, we compare our original image to the twice transformed one. They should be identical, so we can take the average of their difference to be the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65585caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:25:59.086742Z",
     "iopub.status.busy": "2022-10-09T12:25:59.086297Z",
     "iopub.status.idle": "2022-10-09T12:25:59.092178Z",
     "shell.execute_reply": "2022-10-09T12:25:59.091108Z",
     "shell.execute_reply.started": "2022-10-09T12:25:59.086702Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
    "        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "        return LAMBDA * loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d9937",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can also calculate the identity loss of our generators. For example, if we feed a source into our source generator, our output should be identical to the orginal image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00291544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:26:01.710424Z",
     "iopub.status.busy": "2022-10-09T12:26:01.709791Z",
     "iopub.status.idle": "2022-10-09T12:26:01.716097Z",
     "shell.execute_reply": "2022-10-09T12:26:01.714932Z",
     "shell.execute_reply.started": "2022-10-09T12:26:01.710387Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def identity_loss(real_image, same_image, LAMBDA):\n",
    "        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "        return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fec20",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Training the GAN\n",
    "The big moment! We compile our model, and as we inherited from the Keras.Model class, we can use the fit() method to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436303fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:26:05.267943Z",
     "iopub.status.busy": "2022-10-09T12:26:05.266792Z",
     "iopub.status.idle": "2022-10-09T12:26:05.273739Z",
     "shell.execute_reply": "2022-10-09T12:26:05.272842Z",
     "shell.execute_reply.started": "2022-10-09T12:26:05.267862Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    target_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    source_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    target_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    source_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e8a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:26:08.889964Z",
     "iopub.status.busy": "2022-10-09T12:26:08.889225Z",
     "iopub.status.idle": "2022-10-09T12:26:08.912542Z",
     "shell.execute_reply": "2022-10-09T12:26:08.911709Z",
     "shell.execute_reply.started": "2022-10-09T12:26:08.889897Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    cycle_gan_model = CycleGan(\n",
    "        target_generator, source_generator, target_discriminator, source_discriminator\n",
    "    )\n",
    "\n",
    "    cycle_gan_model.compile(\n",
    "        m_gen_optimizer = target_generator_optimizer,\n",
    "        p_gen_optimizer = source_generator_optimizer,\n",
    "        m_disc_optimizer = target_discriminator_optimizer,\n",
    "        p_disc_optimizer = source_discriminator_optimizer,\n",
    "        gen_loss_fn = generator_loss,\n",
    "        disc_loss_fn = discriminator_loss,\n",
    "        cycle_loss_fn = calc_cycle_loss,\n",
    "        identity_loss_fn = identity_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654dcf6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Remember, as we have augmented our dataset, we will need to reduce the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a4256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T12:27:18.985446Z",
     "iopub.status.busy": "2022-10-09T12:27:18.985058Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "n_target_samples = count_data_items(TARGET_FILENAMES)\n",
    "n_source_samples = count_data_items(SOURCE_FILENAMES)\n",
    "\n",
    "epoch_steps = (max(n_target_samples, n_source_samples)//4)\n",
    "#epoch_steps = 600\n",
    "\n",
    "cycle_gan_model.fit(\n",
    "    full_dataset,\n",
    "    epochs=EPOCHS_NUM,\n",
    "    steps_per_epoch=epoch_steps, #error from this variable, hard-code it?\n",
    ")\n",
    "print(FID(fid_source_ds,target_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e78729",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Visualise some outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856b5f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(5, 2, figsize=(12, 12))\n",
    "for i, img in enumerate(source_ds.take(5)):\n",
    "    prediction = photo_generator(img, training=False)[0].numpy()\n",
    "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "    ax[i, 0].imshow(img)\n",
    "    ax[i, 1].imshow(prediction)\n",
    "    ax[i, 0].set_title(\"Input image\")\n",
    "    ax[i, 1].set_title(\"Target style image\")\n",
    "    ax[i, 0].axis(\"off\")\n",
    "    ax[i, 1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7826d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Save the model\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda3bb8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 46.509552,
   "end_time": "2023-05-01T10:43:34.196645",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-01T10:42:47.687093",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
